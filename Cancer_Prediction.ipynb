{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2f07c4a88a8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Capture the original matplotlib rcParams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0m_orig_rc_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Import seaborn objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m \u001b[0m_check_versions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m_check_versions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;31m# Quickfix to ensure Microsoft Visual C++ redistributable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;31m# DLLs are loaded before importing kiwisolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mft2font\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     for modname, minver in [\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\dhruven\\\\cancer_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>benign_0__mal_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  \\\n",
       "0                  0.2654          0.4601                  0.11890   \n",
       "1                  0.1860          0.2750                  0.08902   \n",
       "2                  0.2430          0.3613                  0.08758   \n",
       "3                  0.2575          0.6638                  0.17300   \n",
       "4                  0.1625          0.2364                  0.07678   \n",
       "..                    ...             ...                      ...   \n",
       "564                0.2216          0.2060                  0.07115   \n",
       "565                0.1628          0.2572                  0.06637   \n",
       "566                0.1418          0.2218                  0.07820   \n",
       "567                0.2650          0.4087                  0.12400   \n",
       "568                0.0000          0.2871                  0.07039   \n",
       "\n",
       "     benign_0__mal_1  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "..               ...  \n",
       "564                0  \n",
       "565                0  \n",
       "566                0  \n",
       "567                0  \n",
       "568                1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
       "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
       "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
       "       'worst concave points', 'worst symmetry', 'worst fractal dimension',\n",
       "       'benign_0__mal_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b4c2eb6d93e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(20,30))\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('benign_0__mal_1',axis=1).values\n",
    "y = df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 30)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 30,activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=15,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 455 samples, validate on 114 samples\n",
      "Epoch 1/600\n",
      "455/455 [==============================] - 0s 350us/step - loss: 0.7216 - val_loss: 0.6656\n",
      "Epoch 2/600\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.6689 - val_loss: 0.6354\n",
      "Epoch 3/600\n",
      "455/455 [==============================] - 0s 96us/step - loss: 0.6386 - val_loss: 0.6081\n",
      "Epoch 4/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.6099 - val_loss: 0.5826\n",
      "Epoch 5/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.5880 - val_loss: 0.5540\n",
      "Epoch 6/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.5650 - val_loss: 0.5204\n",
      "Epoch 7/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.5632 - val_loss: 0.4879\n",
      "Epoch 8/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.5412 - val_loss: 0.4666\n",
      "Epoch 9/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.5252 - val_loss: 0.4436\n",
      "Epoch 10/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.4908 - val_loss: 0.4082\n",
      "Epoch 11/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.4917 - val_loss: 0.3878\n",
      "Epoch 12/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.4600 - val_loss: 0.3705\n",
      "Epoch 13/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.4594 - val_loss: 0.3426\n",
      "Epoch 14/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.4225 - val_loss: 0.3166\n",
      "Epoch 15/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.4101 - val_loss: 0.3061\n",
      "Epoch 16/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.3934 - val_loss: 0.2894\n",
      "Epoch 17/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.3878 - val_loss: 0.2592\n",
      "Epoch 18/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.3441 - val_loss: 0.2397\n",
      "Epoch 19/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.3591 - val_loss: 0.2273\n",
      "Epoch 20/600\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.3663 - val_loss: 0.2242\n",
      "Epoch 21/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.3451 - val_loss: 0.2204\n",
      "Epoch 22/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.3239 - val_loss: 0.2062\n",
      "Epoch 23/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.3071 - val_loss: 0.1974\n",
      "Epoch 24/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.3286 - val_loss: 0.1853\n",
      "Epoch 25/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.3017 - val_loss: 0.1820\n",
      "Epoch 26/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.2812 - val_loss: 0.1687\n",
      "Epoch 27/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.2840 - val_loss: 0.1563\n",
      "Epoch 28/600\n",
      "455/455 [==============================] - 0s 69us/step - loss: 0.2804 - val_loss: 0.1537\n",
      "Epoch 29/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.2720 - val_loss: 0.1480\n",
      "Epoch 30/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.2449 - val_loss: 0.1409\n",
      "Epoch 31/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.2815 - val_loss: 0.1332\n",
      "Epoch 32/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.2502 - val_loss: 0.1358\n",
      "Epoch 33/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.2638 - val_loss: 0.1322\n",
      "Epoch 34/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.2440 - val_loss: 0.1303\n",
      "Epoch 35/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.2458 - val_loss: 0.1268\n",
      "Epoch 36/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.2368 - val_loss: 0.1177\n",
      "Epoch 37/600\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.2341 - val_loss: 0.1184\n",
      "Epoch 38/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.2297 - val_loss: 0.1156\n",
      "Epoch 39/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.2173 - val_loss: 0.1120\n",
      "Epoch 40/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.2009 - val_loss: 0.1052\n",
      "Epoch 41/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.2335 - val_loss: 0.1039\n",
      "Epoch 42/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.2284 - val_loss: 0.1032\n",
      "Epoch 43/600\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.2010 - val_loss: 0.0999\n",
      "Epoch 44/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.2024 - val_loss: 0.0987\n",
      "Epoch 45/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.2024 - val_loss: 0.0981\n",
      "Epoch 46/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.2028 - val_loss: 0.0923\n",
      "Epoch 47/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1941 - val_loss: 0.0899\n",
      "Epoch 48/600\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.1848 - val_loss: 0.0861\n",
      "Epoch 49/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.2149 - val_loss: 0.0855\n",
      "Epoch 50/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.1792 - val_loss: 0.0872\n",
      "Epoch 51/600\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.1748 - val_loss: 0.0845\n",
      "Epoch 52/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1856 - val_loss: 0.0813\n",
      "Epoch 53/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.1916 - val_loss: 0.0804\n",
      "Epoch 54/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1659 - val_loss: 0.0799\n",
      "Epoch 55/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.1560 - val_loss: 0.0765\n",
      "Epoch 56/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1718 - val_loss: 0.0765\n",
      "Epoch 57/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1553 - val_loss: 0.0776\n",
      "Epoch 58/600\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.1804 - val_loss: 0.0746\n",
      "Epoch 59/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1712 - val_loss: 0.0746\n",
      "Epoch 60/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1792 - val_loss: 0.0739\n",
      "Epoch 61/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1817 - val_loss: 0.0734\n",
      "Epoch 62/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1445 - val_loss: 0.0744\n",
      "Epoch 63/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1603 - val_loss: 0.0706\n",
      "Epoch 64/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.1590 - val_loss: 0.0703\n",
      "Epoch 65/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.1470 - val_loss: 0.0688\n",
      "Epoch 66/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1729 - val_loss: 0.0685\n",
      "Epoch 67/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1521 - val_loss: 0.0674\n",
      "Epoch 68/600\n",
      "455/455 [==============================] - 0s 73us/step - loss: 0.1426 - val_loss: 0.0677\n",
      "Epoch 69/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.1656 - val_loss: 0.0653\n",
      "Epoch 70/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.1185 - val_loss: 0.0647\n",
      "Epoch 71/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.1382 - val_loss: 0.0631\n",
      "Epoch 72/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1557 - val_loss: 0.0613\n",
      "Epoch 73/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1421 - val_loss: 0.0610\n",
      "Epoch 74/600\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.1430 - val_loss: 0.0623\n",
      "Epoch 75/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.1402 - val_loss: 0.0621\n",
      "Epoch 76/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1560 - val_loss: 0.0620\n",
      "Epoch 77/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.1572 - val_loss: 0.0653\n",
      "Epoch 78/600\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.1177 - val_loss: 0.0628\n",
      "Epoch 79/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1585 - val_loss: 0.0623\n",
      "Epoch 80/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.1467 - val_loss: 0.0623\n",
      "Epoch 81/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.1413 - val_loss: 0.0594\n",
      "Epoch 82/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.1449 - val_loss: 0.0601\n",
      "Epoch 83/600\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.1182 - val_loss: 0.0593\n",
      "Epoch 84/600\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.1222 - val_loss: 0.0572\n",
      "Epoch 85/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1331 - val_loss: 0.0577\n",
      "Epoch 86/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1269 - val_loss: 0.0576\n",
      "Epoch 87/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1145 - val_loss: 0.0571\n",
      "Epoch 88/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1194 - val_loss: 0.0561\n",
      "Epoch 89/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1206 - val_loss: 0.0557\n",
      "Epoch 90/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1046 - val_loss: 0.0573\n",
      "Epoch 91/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.1283 - val_loss: 0.0575\n",
      "Epoch 92/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.1137 - val_loss: 0.0568\n",
      "Epoch 93/600\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.1167 - val_loss: 0.0602\n",
      "Epoch 94/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1176 - val_loss: 0.0592\n",
      "Epoch 95/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.1235 - val_loss: 0.0579\n",
      "Epoch 96/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1234 - val_loss: 0.0576\n",
      "Epoch 97/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.1195 - val_loss: 0.0555\n",
      "Epoch 98/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.1177 - val_loss: 0.0551\n",
      "Epoch 99/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.1267 - val_loss: 0.0551\n",
      "Epoch 100/600\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.1357 - val_loss: 0.0559\n",
      "Epoch 101/600\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.1233 - val_loss: 0.0560\n",
      "Epoch 102/600\n",
      "455/455 [==============================] - 0s 69us/step - loss: 0.1103 - val_loss: 0.0578\n",
      "Epoch 103/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.1306 - val_loss: 0.0548\n",
      "Epoch 104/600\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.1014 - val_loss: 0.0547\n",
      "Epoch 105/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.1110 - val_loss: 0.0522\n",
      "Epoch 106/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1275 - val_loss: 0.0525\n",
      "Epoch 107/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1241 - val_loss: 0.0548\n",
      "Epoch 108/600\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.1174 - val_loss: 0.0537\n",
      "Epoch 109/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1013 - val_loss: 0.0550\n",
      "Epoch 110/600\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.1241 - val_loss: 0.0546\n",
      "Epoch 111/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1067 - val_loss: 0.0586\n",
      "Epoch 112/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1096 - val_loss: 0.0533\n",
      "Epoch 113/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1075 - val_loss: 0.0531\n",
      "Epoch 114/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0986 - val_loss: 0.0521\n",
      "Epoch 115/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.1001 - val_loss: 0.0531\n",
      "Epoch 116/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.1246 - val_loss: 0.0568\n",
      "Epoch 117/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.1106 - val_loss: 0.0610\n",
      "Epoch 118/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1123 - val_loss: 0.0534\n",
      "Epoch 119/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0987 - val_loss: 0.0546\n",
      "Epoch 120/600\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.1175 - val_loss: 0.0530\n",
      "Epoch 121/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1006 - val_loss: 0.0522\n",
      "Epoch 122/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0859 - val_loss: 0.0518\n",
      "Epoch 123/600\n",
      "455/455 [==============================] - 0s 58us/step - loss: 0.0994 - val_loss: 0.0537\n",
      "Epoch 124/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1162 - val_loss: 0.0524\n",
      "Epoch 125/600\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0978 - val_loss: 0.0528\n",
      "Epoch 126/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0844 - val_loss: 0.0519\n",
      "Epoch 127/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0994 - val_loss: 0.0537\n",
      "Epoch 128/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0796 - val_loss: 0.0538\n",
      "Epoch 129/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0965 - val_loss: 0.0513\n",
      "Epoch 130/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0797 - val_loss: 0.0507\n",
      "Epoch 131/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0989 - val_loss: 0.0532\n",
      "Epoch 132/600\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.1095 - val_loss: 0.0528\n",
      "Epoch 133/600\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.1035 - val_loss: 0.0526\n",
      "Epoch 134/600\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.1037 - val_loss: 0.0530\n",
      "Epoch 135/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0782 - val_loss: 0.0545\n",
      "Epoch 136/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.1057 - val_loss: 0.0533\n",
      "Epoch 137/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.1091 - val_loss: 0.0545\n",
      "Epoch 138/600\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.0846 - val_loss: 0.0536\n",
      "Epoch 139/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.1117 - val_loss: 0.0557\n",
      "Epoch 140/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.1170 - val_loss: 0.0527\n",
      "Epoch 141/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0975 - val_loss: 0.0532\n",
      "Epoch 142/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0986 - val_loss: 0.0527\n",
      "Epoch 143/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0761 - val_loss: 0.0538\n",
      "Epoch 144/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1152 - val_loss: 0.0531\n",
      "Epoch 145/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.1073 - val_loss: 0.0524\n",
      "Epoch 146/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0911 - val_loss: 0.0513\n",
      "Epoch 147/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1090 - val_loss: 0.0529\n",
      "Epoch 148/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0927 - val_loss: 0.0531\n",
      "Epoch 149/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1009 - val_loss: 0.0532\n",
      "Epoch 150/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.1028 - val_loss: 0.0512\n",
      "Epoch 151/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0851 - val_loss: 0.0521\n",
      "Epoch 152/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0876 - val_loss: 0.0526\n",
      "Epoch 153/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.1035 - val_loss: 0.0506\n",
      "Epoch 154/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.1019 - val_loss: 0.0515\n",
      "Epoch 155/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0809 - val_loss: 0.0517\n",
      "Epoch 156/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0754 - val_loss: 0.0528\n",
      "Epoch 157/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0840 - val_loss: 0.0508\n",
      "Epoch 158/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0736 - val_loss: 0.0521\n",
      "Epoch 159/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 0s 56us/step - loss: 0.1044 - val_loss: 0.0529\n",
      "Epoch 160/600\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.0726 - val_loss: 0.0565\n",
      "Epoch 161/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0856 - val_loss: 0.0530\n",
      "Epoch 162/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0744 - val_loss: 0.0526\n",
      "Epoch 163/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0813 - val_loss: 0.0525\n",
      "Epoch 164/600\n",
      "455/455 [==============================] - 0s 67us/step - loss: 0.0888 - val_loss: 0.0547\n",
      "Epoch 165/600\n",
      "455/455 [==============================] - 0s 58us/step - loss: 0.0754 - val_loss: 0.0504\n",
      "Epoch 166/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0770 - val_loss: 0.0514\n",
      "Epoch 167/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0842 - val_loss: 0.0509\n",
      "Epoch 168/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0851 - val_loss: 0.0491\n",
      "Epoch 169/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0755 - val_loss: 0.0506\n",
      "Epoch 170/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0928 - val_loss: 0.0530\n",
      "Epoch 171/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0706 - val_loss: 0.0522\n",
      "Epoch 172/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0780 - val_loss: 0.0491\n",
      "Epoch 173/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0742 - val_loss: 0.0503\n",
      "Epoch 174/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0681 - val_loss: 0.0502\n",
      "Epoch 175/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0787 - val_loss: 0.0518\n",
      "Epoch 176/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0821 - val_loss: 0.0512\n",
      "Epoch 177/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0668 - val_loss: 0.0538\n",
      "Epoch 178/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0660 - val_loss: 0.0532\n",
      "Epoch 179/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0820 - val_loss: 0.0521\n",
      "Epoch 180/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0905 - val_loss: 0.0521\n",
      "Epoch 181/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0885 - val_loss: 0.0526\n",
      "Epoch 182/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0770 - val_loss: 0.0526\n",
      "Epoch 183/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0766 - val_loss: 0.0526\n",
      "Epoch 184/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0749 - val_loss: 0.0541\n",
      "Epoch 185/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0797 - val_loss: 0.0557\n",
      "Epoch 186/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0710 - val_loss: 0.0565\n",
      "Epoch 187/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0600 - val_loss: 0.0557\n",
      "Epoch 188/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0717 - val_loss: 0.0540\n",
      "Epoch 189/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0685 - val_loss: 0.0550\n",
      "Epoch 190/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0787 - val_loss: 0.0553\n",
      "Epoch 191/600\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.0936 - val_loss: 0.0573\n",
      "Epoch 192/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0657 - val_loss: 0.0546\n",
      "Epoch 193/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0696 - val_loss: 0.0559\n",
      "Epoch 194/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0739 - val_loss: 0.0552\n",
      "Epoch 195/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0790 - val_loss: 0.0547\n",
      "Epoch 196/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0742 - val_loss: 0.0562\n",
      "Epoch 197/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0704 - val_loss: 0.0564\n",
      "Epoch 198/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0699 - val_loss: 0.0526\n",
      "Epoch 199/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0803 - val_loss: 0.0517\n",
      "Epoch 200/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0882 - val_loss: 0.0511\n",
      "Epoch 201/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0752 - val_loss: 0.0590\n",
      "Epoch 202/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0754 - val_loss: 0.0518\n",
      "Epoch 203/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0688 - val_loss: 0.0542\n",
      "Epoch 204/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0884 - val_loss: 0.0546\n",
      "Epoch 205/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0918 - val_loss: 0.0553\n",
      "Epoch 206/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0689 - val_loss: 0.0563\n",
      "Epoch 207/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0856 - val_loss: 0.0560\n",
      "Epoch 208/600\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.0719 - val_loss: 0.0595\n",
      "Epoch 209/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0709 - val_loss: 0.0547\n",
      "Epoch 210/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0726 - val_loss: 0.0545\n",
      "Epoch 211/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0775 - val_loss: 0.0616\n",
      "Epoch 212/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0855 - val_loss: 0.0578\n",
      "Epoch 213/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0782 - val_loss: 0.0520\n",
      "Epoch 214/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0616 - val_loss: 0.0526\n",
      "Epoch 215/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0710 - val_loss: 0.0532\n",
      "Epoch 216/600\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0680 - val_loss: 0.0555\n",
      "Epoch 217/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0553 - val_loss: 0.0523\n",
      "Epoch 218/600\n",
      "455/455 [==============================] - 0s 58us/step - loss: 0.0599 - val_loss: 0.0548\n",
      "Epoch 219/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0714 - val_loss: 0.0542\n",
      "Epoch 220/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0597 - val_loss: 0.0640\n",
      "Epoch 221/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0666 - val_loss: 0.0522\n",
      "Epoch 222/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0671 - val_loss: 0.0518\n",
      "Epoch 223/600\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.0783 - val_loss: 0.0545\n",
      "Epoch 224/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0609 - val_loss: 0.0489\n",
      "Epoch 225/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0667 - val_loss: 0.0492\n",
      "Epoch 226/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0784 - val_loss: 0.0523\n",
      "Epoch 227/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0648 - val_loss: 0.0514\n",
      "Epoch 228/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0721 - val_loss: 0.0535\n",
      "Epoch 229/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0747 - val_loss: 0.0556\n",
      "Epoch 230/600\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0883 - val_loss: 0.0551\n",
      "Epoch 231/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0560 - val_loss: 0.0537\n",
      "Epoch 232/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0663 - val_loss: 0.0520\n",
      "Epoch 233/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0699 - val_loss: 0.0655\n",
      "Epoch 234/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0893 - val_loss: 0.0516\n",
      "Epoch 235/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0635 - val_loss: 0.0501\n",
      "Epoch 236/600\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.0773 - val_loss: 0.0518\n",
      "Epoch 237/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0609 - val_loss: 0.0544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0581 - val_loss: 0.0557\n",
      "Epoch 239/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0775 - val_loss: 0.0560\n",
      "Epoch 240/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0527\n",
      "Epoch 241/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0876 - val_loss: 0.0561\n",
      "Epoch 242/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0660 - val_loss: 0.0572\n",
      "Epoch 243/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0617 - val_loss: 0.0538\n",
      "Epoch 244/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0589 - val_loss: 0.0524\n",
      "Epoch 245/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0652 - val_loss: 0.0528\n",
      "Epoch 246/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0840 - val_loss: 0.0621\n",
      "Epoch 247/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0637 - val_loss: 0.0541\n",
      "Epoch 248/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0636 - val_loss: 0.0509\n",
      "Epoch 249/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0738 - val_loss: 0.0503\n",
      "Epoch 250/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0746 - val_loss: 0.0553\n",
      "Epoch 251/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0599 - val_loss: 0.0522\n",
      "Epoch 252/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0680 - val_loss: 0.0533\n",
      "Epoch 253/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0761 - val_loss: 0.0542\n",
      "Epoch 254/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0691 - val_loss: 0.0547\n",
      "Epoch 255/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0565 - val_loss: 0.0544\n",
      "Epoch 256/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0600 - val_loss: 0.0548\n",
      "Epoch 257/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0788 - val_loss: 0.0566\n",
      "Epoch 258/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0678 - val_loss: 0.0572\n",
      "Epoch 259/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0682 - val_loss: 0.0569\n",
      "Epoch 260/600\n",
      "455/455 [==============================] - 0s 58us/step - loss: 0.0541 - val_loss: 0.0529\n",
      "Epoch 261/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0526 - val_loss: 0.0548\n",
      "Epoch 262/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0625 - val_loss: 0.0516\n",
      "Epoch 263/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0625 - val_loss: 0.0506\n",
      "Epoch 264/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0534 - val_loss: 0.0523\n",
      "Epoch 265/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0655 - val_loss: 0.0525\n",
      "Epoch 266/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0583 - val_loss: 0.0552\n",
      "Epoch 267/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0596 - val_loss: 0.0557\n",
      "Epoch 268/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0516 - val_loss: 0.0684\n",
      "Epoch 269/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0534 - val_loss: 0.0604\n",
      "Epoch 270/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0589 - val_loss: 0.0652\n",
      "Epoch 271/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0644 - val_loss: 0.0696\n",
      "Epoch 272/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0508 - val_loss: 0.0607\n",
      "Epoch 273/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0566 - val_loss: 0.0574\n",
      "Epoch 274/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0567 - val_loss: 0.0572\n",
      "Epoch 275/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0731 - val_loss: 0.0541\n",
      "Epoch 276/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0601 - val_loss: 0.0526\n",
      "Epoch 277/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0526 - val_loss: 0.0542\n",
      "Epoch 278/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0590 - val_loss: 0.0538\n",
      "Epoch 279/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0526 - val_loss: 0.0562\n",
      "Epoch 280/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0561 - val_loss: 0.0568\n",
      "Epoch 281/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0480 - val_loss: 0.0575\n",
      "Epoch 282/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0468 - val_loss: 0.0598\n",
      "Epoch 283/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0444 - val_loss: 0.0716\n",
      "Epoch 284/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0546 - val_loss: 0.0597\n",
      "Epoch 285/600\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0535 - val_loss: 0.0614\n",
      "Epoch 286/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0503 - val_loss: 0.0620\n",
      "Epoch 287/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0539 - val_loss: 0.0650\n",
      "Epoch 288/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0496 - val_loss: 0.0582\n",
      "Epoch 289/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0499 - val_loss: 0.0590\n",
      "Epoch 290/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0680 - val_loss: 0.0660\n",
      "Epoch 291/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0600 - val_loss: 0.0589\n",
      "Epoch 292/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0660 - val_loss: 0.0601\n",
      "Epoch 293/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0639 - val_loss: 0.0607\n",
      "Epoch 294/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0462 - val_loss: 0.0560\n",
      "Epoch 295/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0594 - val_loss: 0.0571\n",
      "Epoch 296/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0606 - val_loss: 0.0578\n",
      "Epoch 297/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0599 - val_loss: 0.0589\n",
      "Epoch 298/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0538 - val_loss: 0.0583\n",
      "Epoch 299/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0431 - val_loss: 0.0593\n",
      "Epoch 300/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0548 - val_loss: 0.0545\n",
      "Epoch 301/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0446 - val_loss: 0.0543\n",
      "Epoch 302/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0582 - val_loss: 0.0542\n",
      "Epoch 303/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0538 - val_loss: 0.0716\n",
      "Epoch 304/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0591 - val_loss: 0.0706\n",
      "Epoch 305/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0608 - val_loss: 0.0657\n",
      "Epoch 306/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0472 - val_loss: 0.0679\n",
      "Epoch 307/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0591 - val_loss: 0.0704\n",
      "Epoch 308/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0483 - val_loss: 0.0642\n",
      "Epoch 309/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0435 - val_loss: 0.0619\n",
      "Epoch 310/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0488 - val_loss: 0.0601\n",
      "Epoch 311/600\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.0510 - val_loss: 0.0559\n",
      "Epoch 312/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0434 - val_loss: 0.0575\n",
      "Epoch 313/600\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0438 - val_loss: 0.0584\n",
      "Epoch 314/600\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0663 - val_loss: 0.0586\n",
      "Epoch 315/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0599 - val_loss: 0.0641\n",
      "Epoch 316/600\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0588 - val_loss: 0.0633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0524 - val_loss: 0.0650\n",
      "Epoch 318/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0520 - val_loss: 0.0753\n",
      "Epoch 319/600\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0503 - val_loss: 0.0610\n",
      "Epoch 320/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0550 - val_loss: 0.0590\n",
      "Epoch 321/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0659 - val_loss: 0.0644\n",
      "Epoch 322/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0507 - val_loss: 0.0613\n",
      "Epoch 323/600\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0362 - val_loss: 0.0590\n",
      "Epoch 324/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0478 - val_loss: 0.0563\n",
      "Epoch 325/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0531 - val_loss: 0.0546\n",
      "Epoch 326/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0500 - val_loss: 0.0598\n",
      "Epoch 327/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0422 - val_loss: 0.0617\n",
      "Epoch 328/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0487 - val_loss: 0.0615\n",
      "Epoch 329/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0613 - val_loss: 0.0599\n",
      "Epoch 330/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0413 - val_loss: 0.0590\n",
      "Epoch 331/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0394 - val_loss: 0.0571\n",
      "Epoch 332/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0439 - val_loss: 0.0552\n",
      "Epoch 333/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0585 - val_loss: 0.0633\n",
      "Epoch 334/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0518 - val_loss: 0.0579\n",
      "Epoch 335/600\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.0489 - val_loss: 0.0567\n",
      "Epoch 336/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0645 - val_loss: 0.0599\n",
      "Epoch 337/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0518 - val_loss: 0.0579\n",
      "Epoch 338/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0496 - val_loss: 0.0586\n",
      "Epoch 339/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0523 - val_loss: 0.0577\n",
      "Epoch 340/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0429 - val_loss: 0.0555\n",
      "Epoch 341/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0536 - val_loss: 0.0565\n",
      "Epoch 342/600\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0560 - val_loss: 0.0584\n",
      "Epoch 343/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0614 - val_loss: 0.0593\n",
      "Epoch 344/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0417 - val_loss: 0.0571\n",
      "Epoch 345/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0499 - val_loss: 0.0578\n",
      "Epoch 346/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0421 - val_loss: 0.0610\n",
      "Epoch 347/600\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.0529 - val_loss: 0.0613\n",
      "Epoch 348/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0457 - val_loss: 0.0651\n",
      "Epoch 349/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0485 - val_loss: 0.0626\n",
      "Epoch 350/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0416 - val_loss: 0.0610\n",
      "Epoch 351/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0458 - val_loss: 0.0548\n",
      "Epoch 352/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0536 - val_loss: 0.0563\n",
      "Epoch 353/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0572 - val_loss: 0.0654\n",
      "Epoch 354/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0526 - val_loss: 0.0593\n",
      "Epoch 355/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0496 - val_loss: 0.0584\n",
      "Epoch 356/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0434 - val_loss: 0.0588\n",
      "Epoch 357/600\n",
      "455/455 [==============================] - 0s 102us/step - loss: 0.0472 - val_loss: 0.0641\n",
      "Epoch 358/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0513 - val_loss: 0.0615\n",
      "Epoch 359/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0430 - val_loss: 0.0617\n",
      "Epoch 360/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0417 - val_loss: 0.0661\n",
      "Epoch 361/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0354 - val_loss: 0.0664\n",
      "Epoch 362/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0452 - val_loss: 0.0644\n",
      "Epoch 363/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0468 - val_loss: 0.0616\n",
      "Epoch 364/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0445 - val_loss: 0.0605\n",
      "Epoch 365/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0443 - val_loss: 0.0578\n",
      "Epoch 366/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0546 - val_loss: 0.0570\n",
      "Epoch 367/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0577 - val_loss: 0.0590\n",
      "Epoch 368/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0372 - val_loss: 0.0623\n",
      "Epoch 369/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0408 - val_loss: 0.0627\n",
      "Epoch 370/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0558 - val_loss: 0.0644\n",
      "Epoch 371/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0489 - val_loss: 0.0658\n",
      "Epoch 372/600\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0486 - val_loss: 0.0618\n",
      "Epoch 373/600\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.0473 - val_loss: 0.0618\n",
      "Epoch 374/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0380 - val_loss: 0.0624\n",
      "Epoch 375/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0561 - val_loss: 0.0577\n",
      "Epoch 376/600\n",
      "455/455 [==============================] - 0s 58us/step - loss: 0.0408 - val_loss: 0.0554\n",
      "Epoch 377/600\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.0403 - val_loss: 0.0591\n",
      "Epoch 378/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0469 - val_loss: 0.0572\n",
      "Epoch 379/600\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.0508 - val_loss: 0.0684\n",
      "Epoch 380/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0450 - val_loss: 0.0617\n",
      "Epoch 381/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0539 - val_loss: 0.0591\n",
      "Epoch 382/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0598 - val_loss: 0.0707\n",
      "Epoch 383/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0459 - val_loss: 0.0618\n",
      "Epoch 384/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0510 - val_loss: 0.0592\n",
      "Epoch 385/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0418 - val_loss: 0.0593\n",
      "Epoch 386/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0454 - val_loss: 0.0574\n",
      "Epoch 387/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0458 - val_loss: 0.0595\n",
      "Epoch 388/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0471 - val_loss: 0.0613\n",
      "Epoch 389/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0540 - val_loss: 0.0606\n",
      "Epoch 390/600\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.0380 - val_loss: 0.0609\n",
      "Epoch 391/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0462 - val_loss: 0.0602\n",
      "Epoch 392/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0453 - val_loss: 0.0618\n",
      "Epoch 393/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0351 - val_loss: 0.0636\n",
      "Epoch 394/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0404 - val_loss: 0.0606\n",
      "Epoch 395/600\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0408 - val_loss: 0.0609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0351 - val_loss: 0.0687\n",
      "Epoch 397/600\n",
      "455/455 [==============================] - 0s 58us/step - loss: 0.0496 - val_loss: 0.0619\n",
      "Epoch 398/600\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.0418 - val_loss: 0.0646\n",
      "Epoch 399/600\n",
      "455/455 [==============================] - ETA: 0s - loss: 0.009 - 0s 57us/step - loss: 0.0356 - val_loss: 0.0638\n",
      "Epoch 400/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0388 - val_loss: 0.0577\n",
      "Epoch 401/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0453 - val_loss: 0.0566\n",
      "Epoch 402/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0386 - val_loss: 0.0564\n",
      "Epoch 403/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0288 - val_loss: 0.0625\n",
      "Epoch 404/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0522 - val_loss: 0.0759\n",
      "Epoch 405/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0485 - val_loss: 0.0879\n",
      "Epoch 406/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0487 - val_loss: 0.0750\n",
      "Epoch 407/600\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.0431 - val_loss: 0.0630\n",
      "Epoch 408/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0215 - val_loss: 0.0570\n",
      "Epoch 409/600\n",
      "455/455 [==============================] - 0s 58us/step - loss: 0.0428 - val_loss: 0.0578\n",
      "Epoch 410/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0353 - val_loss: 0.0539\n",
      "Epoch 411/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0464 - val_loss: 0.0621\n",
      "Epoch 412/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0399 - val_loss: 0.0610\n",
      "Epoch 413/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0348 - val_loss: 0.0630\n",
      "Epoch 414/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0422 - val_loss: 0.0675\n",
      "Epoch 415/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0309 - val_loss: 0.0588\n",
      "Epoch 416/600\n",
      "455/455 [==============================] - 0s 65us/step - loss: 0.0358 - val_loss: 0.0607\n",
      "Epoch 417/600\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.0447 - val_loss: 0.0614\n",
      "Epoch 418/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0380 - val_loss: 0.0666\n",
      "Epoch 419/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0401 - val_loss: 0.0659\n",
      "Epoch 420/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0431 - val_loss: 0.0626\n",
      "Epoch 421/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0369 - val_loss: 0.0733\n",
      "Epoch 422/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0327 - val_loss: 0.0846\n",
      "Epoch 423/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0490 - val_loss: 0.0681\n",
      "Epoch 424/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0303 - val_loss: 0.0650\n",
      "Epoch 425/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0339 - val_loss: 0.0644\n",
      "Epoch 426/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0569 - val_loss: 0.0623\n",
      "Epoch 427/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0444 - val_loss: 0.0556\n",
      "Epoch 428/600\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.0344 - val_loss: 0.0595\n",
      "Epoch 429/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0466 - val_loss: 0.0594\n",
      "Epoch 430/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0365 - val_loss: 0.0555\n",
      "Epoch 431/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0428 - val_loss: 0.0569\n",
      "Epoch 432/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0327 - val_loss: 0.0717\n",
      "Epoch 433/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0394 - val_loss: 0.0597\n",
      "Epoch 434/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0448 - val_loss: 0.0590\n",
      "Epoch 435/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0445 - val_loss: 0.0663\n",
      "Epoch 436/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0347 - val_loss: 0.0612\n",
      "Epoch 437/600\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.0397 - val_loss: 0.0588\n",
      "Epoch 438/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0392 - val_loss: 0.0598\n",
      "Epoch 439/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0358 - val_loss: 0.0685\n",
      "Epoch 440/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0450 - val_loss: 0.0643\n",
      "Epoch 441/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0403 - val_loss: 0.0643\n",
      "Epoch 442/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0320 - val_loss: 0.0708\n",
      "Epoch 443/600\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.0335 - val_loss: 0.0641\n",
      "Epoch 444/600\n",
      "455/455 [==============================] - 0s 58us/step - loss: 0.0294 - val_loss: 0.0609\n",
      "Epoch 445/600\n",
      "455/455 [==============================] - 0s 58us/step - loss: 0.0437 - val_loss: 0.0618\n",
      "Epoch 446/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0307 - val_loss: 0.0627\n",
      "Epoch 447/600\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.0475 - val_loss: 0.0619\n",
      "Epoch 448/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0438 - val_loss: 0.0787\n",
      "Epoch 449/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0370 - val_loss: 0.0575\n",
      "Epoch 450/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0295 - val_loss: 0.0575\n",
      "Epoch 451/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0436 - val_loss: 0.0586\n",
      "Epoch 452/600\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.0358 - val_loss: 0.0589\n",
      "Epoch 453/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0260 - val_loss: 0.0591\n",
      "Epoch 454/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0406 - val_loss: 0.0711\n",
      "Epoch 455/600\n",
      "455/455 [==============================] - 0s 58us/step - loss: 0.0308 - val_loss: 0.0668\n",
      "Epoch 456/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0381 - val_loss: 0.0629\n",
      "Epoch 457/600\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.0293 - val_loss: 0.0661\n",
      "Epoch 458/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0477 - val_loss: 0.0641\n",
      "Epoch 459/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0262 - val_loss: 0.0573\n",
      "Epoch 460/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0420 - val_loss: 0.0567\n",
      "Epoch 461/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0333 - val_loss: 0.0580\n",
      "Epoch 462/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0424 - val_loss: 0.0612\n",
      "Epoch 463/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0248 - val_loss: 0.0632\n",
      "Epoch 464/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0316 - val_loss: 0.0599\n",
      "Epoch 465/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0494 - val_loss: 0.0617\n",
      "Epoch 466/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0309 - val_loss: 0.0681\n",
      "Epoch 467/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0390 - val_loss: 0.0634\n",
      "Epoch 468/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0298 - val_loss: 0.0617\n",
      "Epoch 469/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0292 - val_loss: 0.0588\n",
      "Epoch 470/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0355 - val_loss: 0.0579\n",
      "Epoch 471/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0483 - val_loss: 0.0630\n",
      "Epoch 472/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0312 - val_loss: 0.0618\n",
      "Epoch 473/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0353 - val_loss: 0.0637\n",
      "Epoch 474/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0315 - val_loss: 0.0646\n",
      "Epoch 475/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0446 - val_loss: 0.0615\n",
      "Epoch 476/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0359 - val_loss: 0.0606\n",
      "Epoch 477/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0245 - val_loss: 0.0701\n",
      "Epoch 478/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0256 - val_loss: 0.0612\n",
      "Epoch 479/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0297 - val_loss: 0.0588\n",
      "Epoch 480/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0411 - val_loss: 0.0712\n",
      "Epoch 481/600\n",
      "455/455 [==============================] - 0s 83us/step - loss: 0.0556 - val_loss: 0.0708\n",
      "Epoch 482/600\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0254 - val_loss: 0.0640\n",
      "Epoch 483/600\n",
      "455/455 [==============================] - 0s 34us/step - loss: 0.0233 - val_loss: 0.0617\n",
      "Epoch 484/600\n",
      "455/455 [==============================] - 0s 34us/step - loss: 0.0462 - val_loss: 0.0638\n",
      "Epoch 485/600\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.0252 - val_loss: 0.0720\n",
      "Epoch 486/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0279 - val_loss: 0.0802\n",
      "Epoch 487/600\n",
      "455/455 [==============================] - 0s 37us/step - loss: 0.0392 - val_loss: 0.0675\n",
      "Epoch 488/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0296 - val_loss: 0.0646\n",
      "Epoch 489/600\n",
      "455/455 [==============================] - 0s 29us/step - loss: 0.0513 - val_loss: 0.0660\n",
      "Epoch 490/600\n",
      "455/455 [==============================] - 0s 101us/step - loss: 0.0310 - val_loss: 0.0813\n",
      "Epoch 491/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0438 - val_loss: 0.0692\n",
      "Epoch 492/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0360 - val_loss: 0.0648\n",
      "Epoch 493/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0462 - val_loss: 0.0656\n",
      "Epoch 494/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0529 - val_loss: 0.0630\n",
      "Epoch 495/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0434 - val_loss: 0.0656\n",
      "Epoch 496/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0313 - val_loss: 0.0638\n",
      "Epoch 497/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0317 - val_loss: 0.0633\n",
      "Epoch 498/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0496 - val_loss: 0.0656\n",
      "Epoch 499/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0247 - val_loss: 0.0660\n",
      "Epoch 500/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0362 - val_loss: 0.0603\n",
      "Epoch 501/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0231 - val_loss: 0.0664\n",
      "Epoch 502/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0213 - val_loss: 0.0665\n",
      "Epoch 503/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0483 - val_loss: 0.0814\n",
      "Epoch 504/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0346 - val_loss: 0.0731\n",
      "Epoch 505/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0392 - val_loss: 0.0631\n",
      "Epoch 506/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0347 - val_loss: 0.0719\n",
      "Epoch 507/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0323 - val_loss: 0.0648\n",
      "Epoch 508/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0388 - val_loss: 0.0695\n",
      "Epoch 509/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0295 - val_loss: 0.0640\n",
      "Epoch 510/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0252 - val_loss: 0.0713\n",
      "Epoch 511/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0332 - val_loss: 0.0716\n",
      "Epoch 512/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0266 - val_loss: 0.0660\n",
      "Epoch 513/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0325 - val_loss: 0.0699\n",
      "Epoch 514/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0460 - val_loss: 0.0617\n",
      "Epoch 515/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0345 - val_loss: 0.0776\n",
      "Epoch 516/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0377 - val_loss: 0.0734\n",
      "Epoch 517/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0407 - val_loss: 0.0710\n",
      "Epoch 518/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0293 - val_loss: 0.0660\n",
      "Epoch 519/600\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0291 - val_loss: 0.0618\n",
      "Epoch 520/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0260 - val_loss: 0.0767\n",
      "Epoch 521/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0355 - val_loss: 0.0665\n",
      "Epoch 522/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0211 - val_loss: 0.0730\n",
      "Epoch 523/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0395 - val_loss: 0.0815\n",
      "Epoch 524/600\n",
      "455/455 [==============================] - 0s 69us/step - loss: 0.0279 - val_loss: 0.0725\n",
      "Epoch 525/600\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0406 - val_loss: 0.0636\n",
      "Epoch 526/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0301 - val_loss: 0.0659\n",
      "Epoch 527/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0372 - val_loss: 0.0672\n",
      "Epoch 528/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0338 - val_loss: 0.0694\n",
      "Epoch 529/600\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0388 - val_loss: 0.0756\n",
      "Epoch 530/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0512 - val_loss: 0.0731\n",
      "Epoch 531/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0285 - val_loss: 0.0675\n",
      "Epoch 532/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0370 - val_loss: 0.0781\n",
      "Epoch 533/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0361 - val_loss: 0.0680\n",
      "Epoch 534/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0172 - val_loss: 0.0704\n",
      "Epoch 535/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0203 - val_loss: 0.0773\n",
      "Epoch 536/600\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0214 - val_loss: 0.0773\n",
      "Epoch 537/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0162 - val_loss: 0.0688\n",
      "Epoch 538/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0240 - val_loss: 0.0696\n",
      "Epoch 539/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0196 - val_loss: 0.0741\n",
      "Epoch 540/600\n",
      "455/455 [==============================] - ETA: 0s - loss: 0.013 - 0s 64us/step - loss: 0.0366 - val_loss: 0.0764\n",
      "Epoch 541/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0156 - val_loss: 0.0707\n",
      "Epoch 542/600\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0297 - val_loss: 0.0685\n",
      "Epoch 543/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0214 - val_loss: 0.0717\n",
      "Epoch 544/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0324 - val_loss: 0.0692\n",
      "Epoch 545/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0342 - val_loss: 0.0688\n",
      "Epoch 546/600\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0266 - val_loss: 0.0667\n",
      "Epoch 547/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0430 - val_loss: 0.0717\n",
      "Epoch 548/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0362 - val_loss: 0.0738\n",
      "Epoch 549/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0320 - val_loss: 0.0828\n",
      "Epoch 550/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0284 - val_loss: 0.0833\n",
      "Epoch 551/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0293 - val_loss: 0.0867\n",
      "Epoch 552/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 0s 57us/step - loss: 0.0250 - val_loss: 0.0712\n",
      "Epoch 553/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0325 - val_loss: 0.0876\n",
      "Epoch 554/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0318 - val_loss: 0.0816\n",
      "Epoch 555/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0343 - val_loss: 0.0760\n",
      "Epoch 556/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0201 - val_loss: 0.0672\n",
      "Epoch 557/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0309 - val_loss: 0.0684\n",
      "Epoch 558/600\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.0269 - val_loss: 0.0740\n",
      "Epoch 559/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0140 - val_loss: 0.0758\n",
      "Epoch 560/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0312 - val_loss: 0.0684\n",
      "Epoch 561/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0290 - val_loss: 0.0681\n",
      "Epoch 562/600\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.0349 - val_loss: 0.0722\n",
      "Epoch 563/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0278 - val_loss: 0.0844\n",
      "Epoch 564/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0274 - val_loss: 0.0783\n",
      "Epoch 565/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0294 - val_loss: 0.0701\n",
      "Epoch 566/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0222 - val_loss: 0.0703\n",
      "Epoch 567/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0242 - val_loss: 0.0785\n",
      "Epoch 568/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0345 - val_loss: 0.0771\n",
      "Epoch 569/600\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0250 - val_loss: 0.0734\n",
      "Epoch 570/600\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0250 - val_loss: 0.0730\n",
      "Epoch 571/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0198 - val_loss: 0.0745\n",
      "Epoch 572/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0245 - val_loss: 0.0758\n",
      "Epoch 573/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0281 - val_loss: 0.0729\n",
      "Epoch 574/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0381 - val_loss: 0.0751\n",
      "Epoch 575/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0270 - val_loss: 0.0673\n",
      "Epoch 576/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0299 - val_loss: 0.0780\n",
      "Epoch 577/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0279 - val_loss: 0.0855\n",
      "Epoch 578/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0358 - val_loss: 0.0743\n",
      "Epoch 579/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0176 - val_loss: 0.0681\n",
      "Epoch 580/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0279 - val_loss: 0.0710\n",
      "Epoch 581/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0320 - val_loss: 0.0717\n",
      "Epoch 582/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0258 - val_loss: 0.0749\n",
      "Epoch 583/600\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0242 - val_loss: 0.0731\n",
      "Epoch 584/600\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0239 - val_loss: 0.0699\n",
      "Epoch 585/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0221 - val_loss: 0.0723\n",
      "Epoch 586/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0267 - val_loss: 0.0727\n",
      "Epoch 587/600\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0211 - val_loss: 0.0778\n",
      "Epoch 588/600\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0212 - val_loss: 0.0688\n",
      "Epoch 589/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0302 - val_loss: 0.0692\n",
      "Epoch 590/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0325 - val_loss: 0.0593\n",
      "Epoch 591/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0215 - val_loss: 0.0635\n",
      "Epoch 592/600\n",
      "455/455 [==============================] - 0s 57us/step - loss: 0.0225 - val_loss: 0.0729\n",
      "Epoch 593/600\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0283 - val_loss: 0.0892\n",
      "Epoch 594/600\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0220 - val_loss: 0.0734\n",
      "Epoch 595/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0155 - val_loss: 0.0723\n",
      "Epoch 596/600\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0220 - val_loss: 0.0731\n",
      "Epoch 597/600\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0212 - val_loss: 0.0760\n",
      "Epoch 598/600\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0350 - val_loss: 0.0772\n",
      "Epoch 599/600\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0156 - val_loss: 0.0658\n",
      "Epoch 600/600\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0384 - val_loss: 0.0655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b2cb06ef28>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=600,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b2caf04be0>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        43\n",
      "           1       0.97      0.97      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.96      0.96       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "[[41  2]\n",
      " [ 2 69]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,pred))\n",
    "\n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 30,activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=15,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 455 samples, validate on 114 samples\n",
      "Epoch 1/250\n",
      "455/455 [==============================] - 0s 615us/step - loss: 0.6481 - val_loss: 0.6231\n",
      "Epoch 2/250\n",
      "455/455 [==============================] - 0s 98us/step - loss: 0.6080 - val_loss: 0.5812\n",
      "Epoch 3/250\n",
      "455/455 [==============================] - 0s 94us/step - loss: 0.5635 - val_loss: 0.5418\n",
      "Epoch 4/250\n",
      "455/455 [==============================] - 0s 85us/step - loss: 0.5267 - val_loss: 0.5034\n",
      "Epoch 5/250\n",
      "455/455 [==============================] - 0s 88us/step - loss: 0.4843 - val_loss: 0.4620\n",
      "Epoch 6/250\n",
      "455/455 [==============================] - 0s 85us/step - loss: 0.4451 - val_loss: 0.4266\n",
      "Epoch 7/250\n",
      "455/455 [==============================] - 0s 85us/step - loss: 0.4191 - val_loss: 0.3935\n",
      "Epoch 8/250\n",
      "455/455 [==============================] - 0s 84us/step - loss: 0.3796 - val_loss: 0.3604\n",
      "Epoch 9/250\n",
      "455/455 [==============================] - 0s 89us/step - loss: 0.3421 - val_loss: 0.3294\n",
      "Epoch 10/250\n",
      "455/455 [==============================] - 0s 85us/step - loss: 0.3114 - val_loss: 0.3137\n",
      "Epoch 11/250\n",
      "455/455 [==============================] - 0s 83us/step - loss: 0.3106 - val_loss: 0.2965\n",
      "Epoch 12/250\n",
      "455/455 [==============================] - 0s 85us/step - loss: 0.2750 - val_loss: 0.2837\n",
      "Epoch 13/250\n",
      "455/455 [==============================] - 0s 77us/step - loss: 0.2631 - val_loss: 0.2664\n",
      "Epoch 14/250\n",
      "455/455 [==============================] - 0s 78us/step - loss: 0.2585 - val_loss: 0.2659\n",
      "Epoch 15/250\n",
      "455/455 [==============================] - 0s 79us/step - loss: 0.2432 - val_loss: 0.2558\n",
      "Epoch 16/250\n",
      "455/455 [==============================] - 0s 74us/step - loss: 0.2336 - val_loss: 0.2427\n",
      "Epoch 17/250\n",
      "455/455 [==============================] - 0s 86us/step - loss: 0.2140 - val_loss: 0.2523\n",
      "Epoch 18/250\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.2151 - val_loss: 0.2390\n",
      "Epoch 19/250\n",
      "455/455 [==============================] - 0s 77us/step - loss: 0.1892 - val_loss: 0.2293\n",
      "Epoch 20/250\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.1898 - val_loss: 0.2217\n",
      "Epoch 21/250\n",
      "455/455 [==============================] - 0s 85us/step - loss: 0.1873 - val_loss: 0.2089\n",
      "Epoch 22/250\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.1697 - val_loss: 0.2032\n",
      "Epoch 23/250\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.1604 - val_loss: 0.1936\n",
      "Epoch 24/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.1611 - val_loss: 0.1856\n",
      "Epoch 25/250\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.1530 - val_loss: 0.1832\n",
      "Epoch 26/250\n",
      "455/455 [==============================] - 0s 74us/step - loss: 0.1640 - val_loss: 0.1774\n",
      "Epoch 27/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.1378 - val_loss: 0.1674\n",
      "Epoch 28/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.1279 - val_loss: 0.1648\n",
      "Epoch 29/250\n",
      "455/455 [==============================] - 0s 79us/step - loss: 0.1286 - val_loss: 0.1497\n",
      "Epoch 30/250\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.1095 - val_loss: 0.1610\n",
      "Epoch 31/250\n",
      "455/455 [==============================] - 0s 79us/step - loss: 0.1388 - val_loss: 0.1578\n",
      "Epoch 32/250\n",
      "455/455 [==============================] - 0s 69us/step - loss: 0.1124 - val_loss: 0.1538\n",
      "Epoch 33/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.0937 - val_loss: 0.1364\n",
      "Epoch 34/250\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0726 - val_loss: 0.1268\n",
      "Epoch 35/250\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0965 - val_loss: 0.1243\n",
      "Epoch 36/250\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0766 - val_loss: 0.1367\n",
      "Epoch 37/250\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0701 - val_loss: 0.1324\n",
      "Epoch 38/250\n",
      "455/455 [==============================] - 0s 77us/step - loss: 0.0713 - val_loss: 0.1264\n",
      "Epoch 39/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.0719 - val_loss: 0.1324\n",
      "Epoch 40/250\n",
      "455/455 [==============================] - 0s 73us/step - loss: 0.0725 - val_loss: 0.1368\n",
      "Epoch 41/250\n",
      "455/455 [==============================] - 0s 77us/step - loss: 0.0963 - val_loss: 0.1474\n",
      "Epoch 42/250\n",
      "455/455 [==============================] - 0s 77us/step - loss: 0.0918 - val_loss: 0.1255\n",
      "Epoch 43/250\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0803 - val_loss: 0.1325\n",
      "Epoch 44/250\n",
      "455/455 [==============================] - 0s 79us/step - loss: 0.0649 - val_loss: 0.1279\n",
      "Epoch 45/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.1111 - val_loss: 0.1306\n",
      "Epoch 46/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.0866 - val_loss: 0.1374\n",
      "Epoch 47/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.0640 - val_loss: 0.1617\n",
      "Epoch 48/250\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0762 - val_loss: 0.1487\n",
      "Epoch 49/250\n",
      "455/455 [==============================] - 0s 76us/step - loss: 0.0802 - val_loss: 0.1353\n",
      "Epoch 50/250\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0599 - val_loss: 0.1361\n",
      "Epoch 51/250\n",
      "455/455 [==============================] - 0s 74us/step - loss: 0.1273 - val_loss: 0.1223\n",
      "Epoch 52/250\n",
      "455/455 [==============================] - 0s 77us/step - loss: 0.0997 - val_loss: 0.1533\n",
      "Epoch 53/250\n",
      "455/455 [==============================] - 0s 83us/step - loss: 0.0825 - val_loss: 0.1107\n",
      "Epoch 54/250\n",
      "455/455 [==============================] - 0s 85us/step - loss: 0.0533 - val_loss: 0.1039\n",
      "Epoch 55/250\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0444 - val_loss: 0.1207\n",
      "Epoch 56/250\n",
      "455/455 [==============================] - 0s 77us/step - loss: 0.0762 - val_loss: 0.1045\n",
      "Epoch 57/250\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0550 - val_loss: 0.1234\n",
      "Epoch 58/250\n",
      "455/455 [==============================] - 0s 79us/step - loss: 0.0436 - val_loss: 0.1245\n",
      "Epoch 59/250\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0387 - val_loss: 0.1250\n",
      "Epoch 60/250\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0414 - val_loss: 0.1214\n",
      "Epoch 61/250\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0344 - val_loss: 0.1269\n",
      "Epoch 62/250\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0684 - val_loss: 0.1468\n",
      "Epoch 63/250\n",
      "455/455 [==============================] - 0s 74us/step - loss: 0.0487 - val_loss: 0.1365\n",
      "Epoch 64/250\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0648 - val_loss: 0.1200\n",
      "Epoch 65/250\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0503 - val_loss: 0.1323\n",
      "Epoch 66/250\n",
      "455/455 [==============================] - 0s 69us/step - loss: 0.0808 - val_loss: 0.1311\n",
      "Epoch 67/250\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0687 - val_loss: 0.1239\n",
      "Epoch 68/250\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0692 - val_loss: 0.1198\n",
      "Epoch 69/250\n",
      "455/455 [==============================] - 0s 67us/step - loss: 0.0274 - val_loss: 0.1134\n",
      "Epoch 70/250\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0500 - val_loss: 0.1095\n",
      "Epoch 71/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.0960 - val_loss: 0.1011\n",
      "Epoch 72/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.0557 - val_loss: 0.1232\n",
      "Epoch 73/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.0721 - val_loss: 0.1663\n",
      "Epoch 74/250\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0426 - val_loss: 0.1553\n",
      "Epoch 75/250\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0365 - val_loss: 0.1392\n",
      "Epoch 76/250\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0335 - val_loss: 0.1463\n",
      "Epoch 77/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.0523 - val_loss: 0.1561\n",
      "Epoch 78/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.0309 - val_loss: 0.1404\n",
      "Epoch 79/250\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0411 - val_loss: 0.1484\n",
      "Epoch 80/250\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0369 - val_loss: 0.1511\n",
      "Epoch 81/250\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0364 - val_loss: 0.1418\n",
      "Epoch 82/250\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0318 - val_loss: 0.1357\n",
      "Epoch 83/250\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0582 - val_loss: 0.1253\n",
      "Epoch 84/250\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0341 - val_loss: 0.1356\n",
      "Epoch 85/250\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0646 - val_loss: 0.1124\n",
      "Epoch 86/250\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0606 - val_loss: 0.1131\n",
      "Epoch 87/250\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0357 - val_loss: 0.1262\n",
      "Epoch 88/250\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0203 - val_loss: 0.1317\n",
      "Epoch 89/250\n",
      "455/455 [==============================] - 0s 75us/step - loss: 0.0364 - val_loss: 0.1242\n",
      "Epoch 90/250\n",
      "455/455 [==============================] - 0s 71us/step - loss: 0.0604 - val_loss: 0.1355\n",
      "Epoch 91/250\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0365 - val_loss: 0.1458\n",
      "Epoch 00091: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b2cb3ef3c8>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=250,validation_data=(X_test,y_test),callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b2cbb66390>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        43\n",
      "           1       0.97      0.97      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.96      0.96       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41  2]\n",
      " [ 2 69]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = df.drop('benign_0__mal_1',axis=1).iloc[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = scaler.transform(patient.values.reshape(-1,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
